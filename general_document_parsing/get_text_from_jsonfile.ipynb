{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pure texts from generated json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract information from texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_info_ext:\n",
    "    def __init__(self, text=None):\n",
    "        self.text = text\n",
    "\n",
    "        date_patterns = [r\"(?<=\\s)\\d{4}\", r\"(?<=\\s)\\d{4}\\s[A-Za-z]{3,9}\\s\\d{1,2}\", r\"(?<=\\s)\\d{1,2}\\s[A-Za-z]{3,9}\\s\\d{4}\", r\"(?<=\\s)\\d{4}\\s[A-Za-z]{3}\\.\\s\\d{1,2}\",\n",
    "                         r\"(?<=\\s)\\d{1,2}\\s[A-Za-z]{3}\\.\\s\\d{4}\", r\"(?<=\\s)\\d{4}-\\d{1,2}-\\d{1,2}\", r\"(?<=\\s)\\d{1,2}-\\d{1,2}-\\d{4}\", r\"(?<=\\s)\\d{4}\\/\\d{1,2}\\/\\d{1,2}\", r\"(?<=\\s)\\d{1,2}\\/\\d{1,2}\\/\\d{4}\"]\n",
    "\n",
    "        \"\"\" numbers_patterns \"\"\"\n",
    "        number_patterns = [r\"\\-?\\d+\", r\"\\-?\\d+%\", r\"\\-?\\d{1,3},\\d{3}\", r\"\\-?\\d{1,3},\\d{3}%\", r\"\\-?\\d{1,3},\\d{3},\\d{3}\", r\"\\-?\\d{1,3},\\d{3},\\d{3}%\", r\"\\-?\\d{1,3},\\d{3},\\d{3},\\d{3}\", r\"\\-?\\d{1,3},\\d{3},\\d{3},\\d{3}%\", r\"\\-?\\d+\\.\\d+\",\n",
    "                           r\"\\-?\\d+\\.\\d+%\", r\"\\-?\\d{1,3},\\d{3}\\.\\d+\", r\"\\-?\\d{1,3},\\d{3}\\.\\d+%\", r\"\\-?\\d{1,3},\\d{3},\\d{3}\\.\\d+\", r\"\\-?\\d{1,3},\\d{3},\\d{3}\\.\\d+%\", r\"\\-?\\d{1,3},\\d{3},\\d{3},\\d{3}\\.\\d+\", r\"\\-?\\d{1,3},\\d{3},\\d{3},\\d{3}\\.\\d+%\"]\n",
    "\n",
    "        \"\"\" keywords \"\"\"\n",
    "        keyword_patterns = [r\"GHG\", r\"Greenhouse gases\", r\"Greenhouse gases [(]GHG[)]\", r\"GHG emissions?\", r\"GHG scope 1 emissions?\", r\"GHG scope 2 emmisions?\", r\"scope 1\", r\"scope 2\", r\"scope 3\", r\"intensity\", r\"emission intensity\", r\"GHG emission intensity\", r\"green house gas\", r\"non-hazardous wastes?\", r\"hazardous wastes?\", r\"hazardous and non-hazardous wastes?\", r\"other non-hazardous wastes?\", r\"total carbon emissions?\", r\"carbon emissions?\", r\"sulphur oxides?\", r\"sulphur oxides? [(]SOx[)]\", r\"nitrogen oxides?\", r\"nitrogen oxides? [(]NOx[)]\", r\"particulate matters?\", r\"particulate matters? [(]PM[)]\", r\"sulfur dioxides?\", r\"nitrogen dioxides?\", r\"chemical oxygen demands?\", r\"chemical oxygen demands? [(]COD[)]\", r\"carbon dioxide equivalents?\",\n",
    "                            r\"tCO2e\", r\"carbon dioxide equivalents? [(]tCO2e[)]\", r\"carbon dioxides?\", r\"CO2\", r\"carbon dioxides? [(]CO2[)]\", r\"methanes?\", r\"methanes? [(]CH4[)]\", r\"methanol\", r\"nitrous oxides?\", r\"nitrous oxides? [(]N2O[)]\", r\"ammonia nitrogen\", r\"ammonia-nitrogen\", r\"HFC\", r\"PFC\", r\"Volatile organic compounds?\", r\"O3\", r\"Ozone [(]O3[)]\", r\"PM10\", r\"Coarse particles? [(]PM10[)]\", r\"Dust [(]total measurable particles?[)]\", r\"PM2.5\", r\"Small particles? dust [(]PM2.5[)]\", r\"Lead [(]Pb[)]\", r\"C20H12\", r\"Benzo[(]a[)] pyrene [(]C20H12[)]\", r\"coal gangues?\", r\"coal fly ash\", r\"cinders?\", r\"chemical wastes?\", r\"muds?\", r\"rocks?\", r\"industrial water\", r\"mine water\", r\"emissions\", r\"oily sludges?\", r\"oily wastes?\"]\n",
    "\n",
    "        \"\"\" measurements \"\"\"\n",
    "        measurement_patterns = [r\"Mwh\", r\"Megawatt hours\", r\"Megawatt hours [(]MWh[)]\", r\"tonnes?\", r\"tons?\",\n",
    "                                r\"metric tonnes?\", r\"metric tons?\", r\"kgs?\", r\"million tonnes?\", r\"million tons?\", r\"cubic meters?\"]\n",
    "        intensity_patterns = [r\"kg/m2\", r\"kg/employee\",\n",
    "                              r\"tCO2e/m2\", r\"tCO2e/employee\"]\n",
    "        currency_patterns = [r\"RMB\", r\"USD\", r\"HKD\"]\n",
    "\n",
    "        # \"\"\" Verbs, conjunctions, prepositions \"\"\"\n",
    "        # # r\"(?<=\\s)WORD(?=\\s)\" to match VCP keyword exactly, not as a part of a long word\n",
    "        # VCP_patterns = [r\"(?<=\\s)of(?=\\s)\", r\"(?<=\\s)and(?=\\s)\", r\"(?<=\\s)or(?=\\s)\", r\"(?<=\\s)am(?=\\s)\",\n",
    "        #                 r\"(?<=\\s)is(?=\\s)\", r\"(?<=\\s)are(?=\\s)\", r\"(?<=\\s)was(?=\\s)\", r\"(?<=\\s)were(?=\\s)\"]\n",
    "\n",
    "        \"\"\" all patterns together \"\"\"\n",
    "        # patterns_list = keyword_patterns + number_patterns + date_patterns + measurement_patterns + intensity_patterns + VCP_patterns\n",
    "        patterns_list = keyword_patterns + number_patterns + date_patterns + \\\n",
    "            measurement_patterns + intensity_patterns + currency_patterns\n",
    "\n",
    "        self.text_data, self.matchlist, self.result = self.text_information_extraction(\n",
    "            text, number_patterns, keyword_patterns, measurement_patterns, patterns_list)\n",
    "\n",
    "    def text_information_extraction(self, text, number_patterns, keyword_patterns, measurement_patterns, patterns_list):\n",
    "        if len(text) == 0:\n",
    "            print(\"Please give a valid text.\")\n",
    "            return None\n",
    "\n",
    "        results_df = pd.DataFrame(columns=['number', 'num_start_pos', 'num_end_pos', 'measurement',\n",
    "                                           'meas_start_pos', 'meas_end_pos', 'keyword', 'key_start_pos', 'key_end_pos'])\n",
    "\n",
    "        match_list = self.find_all_patterns(patterns_list, text)\n",
    "        matchlist_norddt = self.matchlist_noredundant(match_list)\n",
    "\n",
    "        rule_list = self.rule1_number_meas(\n",
    "            number_patterns, measurement_patterns, keyword_patterns, matchlist_norddt)\n",
    "        temp_df = self.apply_rule1(rule_list, matchlist_norddt)\n",
    "\n",
    "        if (temp_df is not None):\n",
    "            results_df = pd.concat(\n",
    "                [results_df, temp_df], ignore_index=True)\n",
    "\n",
    "        rule_list = self.rule2_rep3time_num_meas(\n",
    "            number_patterns, measurement_patterns, keyword_patterns, matchlist_norddt)\n",
    "\n",
    "        temp_df = self.apply_rule2(rule_list, matchlist_norddt)\n",
    "        if (temp_df is not None):\n",
    "            results_df = pd.concat(\n",
    "                [results_df, temp_df], ignore_index=True)\n",
    "\n",
    "        return text, matchlist_norddt, results_df\n",
    "\n",
    "    def find_all_patterns(self, patterns, text):\n",
    "        \"\"\"find all patterns in pattern-list for given text\"\"\"\n",
    "        match_list = []\n",
    "        for pattern in patterns:\n",
    "            text_temp = text\n",
    "            text_lenth = len(text_temp)\n",
    "            start_pos = 0\n",
    "            end_pos = 0\n",
    "            index_shift = 0\n",
    "            while text_lenth > 0:\n",
    "                # not case sensitive\n",
    "                result = re.search(pattern, text_temp, re.IGNORECASE)\n",
    "                if result is None:\n",
    "                    break\n",
    "                start_pos = result.start()\n",
    "                end_pos = result.end() - 1\n",
    "                match_list.append(\n",
    "                    [result[0].strip(), start_pos + index_shift, end_pos + index_shift])\n",
    "                index_shift += end_pos + 1\n",
    "                text_temp = text_temp[end_pos + 1:]\n",
    "                text_lenth = len(text_temp)\n",
    "\n",
    "        return match_list\n",
    "\n",
    "    def matchlist_noredundant(self, match_list):\n",
    "        results = []\n",
    "        length = len(match_list)\n",
    "        if len(match_list) == 0:\n",
    "            return None\n",
    "        if length == 1:\n",
    "            return self.output_DataFrame(match_list)\n",
    "        else:\n",
    "            for i in range(length):\n",
    "                for j in range(length):\n",
    "                    if (i != j) and (self.is_first_redundant(match_list[i], match_list[j])):\n",
    "                        break\n",
    "                    if (j == length - 1) and (match_list[i] not in results):\n",
    "                        results.append(match_list[i])\n",
    "\n",
    "        return self.output_DataFrame(results)\n",
    "\n",
    "    def is_first_redundant(self, item1, item2):\n",
    "        return (item1[1] >= item2[1]) and (item1[2] < item2[2]) or (item1[1] > item2[1]) and (item1[2] <= item2[2])\n",
    "\n",
    "    def output_DataFrame(self, match_list):\n",
    "        if len(match_list) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            results_df = pd.DataFrame(match_list)\n",
    "            results_df.columns = ['keyword', 'start_pos', 'end_pos']\n",
    "            results_df.sort_values(\n",
    "                by='start_pos', ascending=True, inplace=True, ignore_index=True)\n",
    "            return results_df\n",
    "\n",
    "    def is_alnumber(self, number_patterns, test_string):\n",
    "        \"\"\" check whether the string is purely numbers \"\"\"\n",
    "        result = self.find_all_patterns(number_patterns, test_string)\n",
    "        if len(result) == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def is_keyword_in_patterns(self, patterns, keyword):\n",
    "        \"\"\" #check the type of keyword \"\"\"\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, keyword, re.IGNORECASE) is not None:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def rule1_number_meas(self, number_patterns, measurement_patterns, keyword_patterns, matchlist_df):\n",
    "        if matchlist_df is None:\n",
    "            return []\n",
    "\n",
    "        if matchlist_df.shape[0] <= 2:\n",
    "            return []\n",
    "\n",
    "        result = []\n",
    "        for i in range(matchlist_df.shape[0]-2):\n",
    "            item1 = matchlist_df.iloc[i]\n",
    "            item2 = matchlist_df.iloc[i+1]\n",
    "            item3 = matchlist_df.iloc[i+2]\n",
    "\n",
    "            if (abs(item2['start_pos'] - item1['end_pos']) == 2) and (abs(item3['start_pos'] - item2['end_pos']) == 5) and (self.is_keyword_in_patterns(number_patterns, item1[0])) and (self.is_keyword_in_patterns(measurement_patterns, item2[0])) and (self.is_keyword_in_patterns(keyword_patterns, item3[0])):\n",
    "                result.append(i)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def apply_rule1(self, rule_list, matchlist_df):\n",
    "\n",
    "        if len(rule_list) == 0:\n",
    "            return None\n",
    "\n",
    "        matched_df = []\n",
    "\n",
    "        for index in rule_list:\n",
    "            item1 = matchlist_df.iloc[index]\n",
    "            item2 = matchlist_df.iloc[index+1]\n",
    "            item3 = matchlist_df.iloc[index+2]\n",
    "            matched_item = [item1[0], item1[1], item1[2], item2[0],\n",
    "                            item2[1], item2[2], item3[0], item3[1], item3[2]]\n",
    "            matched_df.append(matched_item)\n",
    "\n",
    "        matched_df = pd.DataFrame(matched_df, columns=['number', 'num_start_pos', 'num_end_pos',\n",
    "                                                       'measurement', 'meas_start_pos', 'meas_end_pos', 'keyword', 'key_start_pos', 'key_end_pos'])\n",
    "\n",
    "        return matched_df\n",
    "\n",
    "    # rule2: number + measurement + \"of\" + keywords\n",
    "\n",
    "    def rule2_rep3time_num_meas(self, number_patterns, measurement_patterns, keyword_patterns, matchlist_df):\n",
    "        if matchlist_df is None:\n",
    "            return []\n",
    "\n",
    "        if matchlist_df.shape[0] <= 8:\n",
    "            return []\n",
    "\n",
    "        result = []\n",
    "        for i in range(matchlist_df.shape[0]-8):\n",
    "            item1 = matchlist_df.iloc[i]\n",
    "            item2 = matchlist_df.iloc[i+1]\n",
    "            item3 = matchlist_df.iloc[i+2]\n",
    "            item4 = matchlist_df.iloc[i+3]\n",
    "            item5 = matchlist_df.iloc[i+4]\n",
    "            item6 = matchlist_df.iloc[i+5]\n",
    "            item7 = matchlist_df.iloc[i+6]\n",
    "            item8 = matchlist_df.iloc[i+7]\n",
    "            item9 = matchlist_df.iloc[i+8]\n",
    "\n",
    "            if (abs(item2['start_pos'] - item1['end_pos']) == 2) and (abs(item4['start_pos'] - item3['end_pos']) == 2) and (abs(item6['start_pos'] - item5['end_pos']) == 2) and (abs(item7['start_pos'] - item6['end_pos']) == 5) and (self.is_keyword_in_patterns(number_patterns, item1[0])) and (self.is_keyword_in_patterns(measurement_patterns, item2[0])) and \\\n",
    "                (self.is_keyword_in_patterns(number_patterns, item3[0])) and (self.is_keyword_in_patterns(measurement_patterns, item4[0])) and \\\n",
    "                (self.is_keyword_in_patterns(number_patterns, item5[0])) and (self.is_keyword_in_patterns(measurement_patterns, item6[0])) and \\\n",
    "                    (self.is_keyword_in_patterns(keyword_patterns, item7[0])) and (self.is_keyword_in_patterns(keyword_patterns, item8[0])) and (self.is_keyword_in_patterns(keyword_patterns, item9[0])):\n",
    "                result.append(i)\n",
    "                print(i)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def apply_rule2(self, rule_list, matchlist_df):\n",
    "\n",
    "        if len(rule_list) == 0:\n",
    "            return None\n",
    "\n",
    "        matched_df = []\n",
    "\n",
    "        for index in rule_list:\n",
    "            item1 = matchlist_df.iloc[index]\n",
    "            item2 = matchlist_df.iloc[index+1]\n",
    "            item3 = matchlist_df.iloc[index+2]\n",
    "            item4 = matchlist_df.iloc[index+3]\n",
    "            item5 = matchlist_df.iloc[index+4]\n",
    "            item6 = matchlist_df.iloc[index+5]\n",
    "            item7 = matchlist_df.iloc[index+6]\n",
    "            item8 = matchlist_df.iloc[index+7]\n",
    "            item9 = matchlist_df.iloc[index+8]\n",
    "\n",
    "            matched_item1 = [item1[0], item1[1], item1[2], item2[0],\n",
    "                             item2[1], item2[2], item7[0], item7[1], item7[2]]\n",
    "            matched_item2 = [item3[0], item3[1], item3[2], item4[0],\n",
    "                             item4[1], item4[2], item8[0], item8[1], item8[2]]\n",
    "            matched_item3 = [item5[0], item5[1], item5[2], item6[0],\n",
    "                             item6[1], item6[2], item9[0], item9[1], item9[2]]\n",
    "\n",
    "            matched_df.append(matched_item1)\n",
    "            matched_df.append(matched_item2)\n",
    "            matched_df.append(matched_item3)\n",
    "\n",
    "        matched_df = pd.DataFrame(matched_df, columns=['number', 'num_start_pos', 'num_end_pos',\n",
    "                                                       'measurement', 'meas_start_pos', 'meas_end_pos', 'keyword', 'key_start_pos', 'key_end_pos'])\n",
    "\n",
    "        return matched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_key(text_dict, keyword):\n",
    "    \"\"\"if give dict has the key of kewword\"\"\"\n",
    "    return keyword in text_dict.keys()\n",
    "\n",
    "\n",
    "def get_text_from_json(json_file_path):\n",
    "    \"\"\"get pure texts from generated json file\n",
    "\n",
    "    Args:\n",
    "        json_file_path (json file): generated json file from doc parsing\n",
    "    Output: \n",
    "        List of pure texts from generated json file\n",
    "    \"\"\"\n",
    "    pure_texts = []\n",
    "\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        text_data = json.load(f)\n",
    "\n",
    "    for item in text_data['content']:\n",
    "        if has_key(item, 'paragraph'):\n",
    "            pure_texts.append(re.sub('\\s', ' ', item['paragraph']))\n",
    "        if has_key(item, 'child_content'):\n",
    "            if item['child_content'] is not None:\n",
    "                for child_item in item['child_content']:\n",
    "                    if has_key(child_item, 'paragraph'):\n",
    "                        pure_texts.append(\n",
    "                            re.sub('\\s', ' ', child_item['paragraph']))\n",
    "\n",
    "    return pure_texts\n",
    "\n",
    "\n",
    "def extract_text_info(texts):\n",
    "    \"\"\"extract information from list of texts\n",
    "\n",
    "    Args:\n",
    "        texts (list of string)\n",
    "    Output: \n",
    "        DataFrame of extracted information from texts\n",
    "    \"\"\"\n",
    "    results_df = pd.DataFrame(columns=['number', 'num_start_pos', 'num_end_pos', 'measurement',\n",
    "                                       'meas_start_pos', 'meas_end_pos', 'keyword', 'key_start_pos', 'key_end_pos'])\n",
    "\n",
    "    for text in texts:\n",
    "        model = text_info_ext(text)\n",
    "        if model.result.shape[0] > 0:\n",
    "            results_df = pd.concat(\n",
    "                [results_df, model.result], ignore_index=True)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_info(texts):\n",
    "    \"\"\"extract information from list of texts\n",
    "\n",
    "    Args:\n",
    "        texts (list of string)\n",
    "    Output: \n",
    "        DataFrame of extracted information from texts\n",
    "    \"\"\"\n",
    "    # results_df = pd.DataFrame(columns=['number', 'num_start_pos', 'num_end_pos', 'measurement',\n",
    "    #                                    'meas_start_pos', 'meas_end_pos', 'keyword', 'key_start_pos', 'key_end_pos'])\n",
    "    \n",
    "    final_results = []\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for text in texts:\n",
    "        results = dict()\n",
    "        results['text'] = text\n",
    "        results_df = pd.DataFrame(columns=['number', 'num_start_pos', 'num_end_pos', 'measurement',\n",
    "                                           'meas_start_pos', 'meas_end_pos', 'keyword', 'key_start_pos', 'key_end_pos'])\n",
    "        model = text_info_ext(text)\n",
    "        if model.result.shape[0] > 0:\n",
    "            results_df = pd.concat(\n",
    "                [results_df, model.result], ignore_index=True)\n",
    "\n",
    "            results['ext_info'] = results_df\n",
    "\n",
    "            final_results.append(results)\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/json/巨濤海洋石油服務_Environmental,SocialandGovernanceReport2020.json'\n",
    "texts = get_text_from_json(file_path)\n",
    "\n",
    "results = extract_text_info(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'The Group is committed to reducing waste during constructions. It had formulated  the Waste Safety   Management Regulations  to regulate the recycle, storage and treatment of waste of the Group. During the   year, the Group generated 3,816 metric tons of non-hazardous waste, mainly including waste metal and steel,   domestic waste, waste cardboard boxes, waste timber and office paper. All non-hazardous wastes were   collected and transported by recyclers.    During the year, the Group generated 566 metric tons of hazardous waste',\n",
       "  'ext_info':   number num_start_pos num_end_pos  measurement meas_start_pos meas_end_pos  \\\n",
       "  0  3,816           234         238  metric tons            240          250   \n",
       "  1    566           500         502  metric tons            504          514   \n",
       "  \n",
       "                 keyword key_start_pos key_end_pos  \n",
       "  0  non-hazardous waste           255         273  \n",
       "  1      hazardous waste           519         533  },\n",
       " {'text': 'The Group is committed to reducing waste during constructions. It had formulated  the Waste Safety   Management Regulations  to regulate the recycle, storage and treatment of waste of the Group. During the   year, the Group generated 3,816 metric tons of non-hazardous waste, mainly including waste metal and steel,   domestic waste, waste cardboard boxes, waste timber and office paper. All non-hazardous wastes were   collected and transported by recyclers.    During the year, the Group generated 566 metric tons of hazardous waste',\n",
       "  'ext_info':   number num_start_pos num_end_pos  measurement meas_start_pos meas_end_pos  \\\n",
       "  0  3,816           234         238  metric tons            240          250   \n",
       "  1    566           500         502  metric tons            504          514   \n",
       "  \n",
       "                 keyword key_start_pos key_end_pos  \n",
       "  0  non-hazardous waste           255         273  \n",
       "  1      hazardous waste           519         533  }]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' During the year, the Group’s total carbon emissions were 38,124 metric tons of carbon dioxide equivalent, mainly from the purchased electricity, accounting for 68% of the total carbon emissions. Compared with last year’s performance, the Group’s total GHG emissions increased 87%, which was mainly generated by the increasing work load of construction sites. The GHG emission intensity by the number of employees is 11.1, while the intensity by RMB 1,000,000 is 10.5. ',\n",
       " ' The Group is committed to reducing waste during constructions. It had formulated the Waste Safety Management Regulations to regulate the recycle, storage and treatment of waste of the Group. During the year, the Group generated 3,816 metric tons of non-hazardous waste, mainly including waste metal and steel, domestic waste, waste cardboard boxes, waste timber and office paper. All non-hazardous wastes were collected and transported by recyclers. ',\n",
       " ' During the year, the Group generated 566 metric tons of hazardous waste 13 , including waste paint residue, mineral oil, paint buckets, lubricants, and toner cartridges. All waste were collected and processed by qualified contractors. ']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts=[]\n",
    "text = \"\"\"\n",
    "During the year, the Group’s total carbon emissions were 38,124 metric tons of carbon dioxide equivalent,\n",
    "mainly from the purchased electricity, accounting for 68% of the total carbon emissions. Compared with last\n",
    "year’s performance, the Group’s total GHG emissions increased 87%, which was mainly generated by the\n",
    "increasing work load of construction sites. The GHG emission intensity by the number of employees is 11.1,\n",
    "while the intensity by RMB 1,000,000 is 10.5.\n",
    "\"\"\"\n",
    "test_texts.append(re.sub('\\s', ' ', text))\n",
    "\n",
    "text = \"\"\"\n",
    "The Group is committed to reducing waste during constructions. It had formulated the Waste Safety\n",
    "Management Regulations to regulate the recycle, storage and treatment of waste of the Group. During the\n",
    "year, the Group generated 3,816 metric tons of non-hazardous waste, mainly including waste metal and steel,\n",
    "domestic waste, waste cardboard boxes, waste timber and office paper. All non-hazardous wastes were\n",
    "collected and transported by recyclers.\n",
    "\"\"\"\n",
    "test_texts.append(re.sub('\\s', ' ', text))\n",
    "\n",
    "text = \"\"\"\n",
    "During the year, the Group generated 566 metric tons of hazardous waste 13 , including waste paint residue,\n",
    "mineral oil, paint buckets, lubricants, and toner cartridges. All waste were collected and processed by\n",
    "qualified contractors.\n",
    "\"\"\"\n",
    "test_texts.append(re.sub('\\s', ' ', text))\n",
    "\n",
    "print(len(test_texts))\n",
    "test_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number num_start_pos num_end_pos  measurement meas_start_pos meas_end_pos  \\\n",
      "0  38,124            58          63  metric tons             65           75   \n",
      "1   3,816           229         233  metric tons            235          245   \n",
      "2     566            38          40  metric tons             42           52   \n",
      "\n",
      "                     keyword key_start_pos key_end_pos  \n",
      "0  carbon dioxide equivalent            80         104  \n",
      "1        non-hazardous waste           250         268  \n",
      "2            hazardous waste            57          71  \n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "results = extract_text_info(test_texts)\n",
    "\n",
    "print(results)\n",
    "print(type(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"0\":{\"number\":\"38,124\",\"num_start_pos\":58,\"num_end_pos\":63,\"measurement\":\"metric tons\",\"meas_start_pos\":65,\"meas_end_pos\":75,\"keyword\":\"carbon dioxide equivalent\",\"key_start_pos\":80,\"key_end_pos\":104},\"1\":{\"number\":\"3,816\",\"num_start_pos\":229,\"num_end_pos\":233,\"measurement\":\"metric tons\",\"meas_start_pos\":235,\"meas_end_pos\":245,\"keyword\":\"non-hazardous waste\",\"key_start_pos\":250,\"key_end_pos\":268},\"2\":{\"number\":\"566\",\"num_start_pos\":38,\"num_end_pos\":40,\"measurement\":\"metric tons\",\"meas_start_pos\":42,\"meas_end_pos\":52,\"keyword\":\"hazardous waste\",\"key_start_pos\":57,\"key_end_pos\":71}}'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js = results.to_json(orient='index')\n",
    "print(len(js))\n",
    "js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'number': '38,124',\n",
       "  'num_start_pos': 58,\n",
       "  'num_end_pos': 63,\n",
       "  'measurement': 'metric tons',\n",
       "  'meas_start_pos': 65,\n",
       "  'meas_end_pos': 75,\n",
       "  'keyword': 'carbon dioxide equivalent',\n",
       "  'key_start_pos': 80,\n",
       "  'key_end_pos': 104},\n",
       " '1': {'number': '3,816',\n",
       "  'num_start_pos': 229,\n",
       "  'num_end_pos': 233,\n",
       "  'measurement': 'metric tons',\n",
       "  'meas_start_pos': 235,\n",
       "  'meas_end_pos': 245,\n",
       "  'keyword': 'non-hazardous waste',\n",
       "  'key_start_pos': 250,\n",
       "  'key_end_pos': 268},\n",
       " '2': {'number': '566',\n",
       "  'num_start_pos': 38,\n",
       "  'num_end_pos': 40,\n",
       "  'measurement': 'metric tons',\n",
       "  'meas_start_pos': 42,\n",
       "  'meas_end_pos': 52,\n",
       "  'keyword': 'hazardous waste',\n",
       "  'key_start_pos': 57,\n",
       "  'key_end_pos': 71}}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_resluts = json.loads(js)\n",
    "final_resluts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"success\": true, \"output\": {\"0\": {\"number\": \"38,124\", \"num_start_pos\": 58, \"num_end_pos\": 63, \"measurement\": \"metric tons\", \"meas_start_pos\": 65, \"meas_end_pos\": 75, \"keyword\": \"carbon dioxide equivalent\", \"key_start_pos\": 80, \"key_end_pos\": 104}, \"1\": {\"number\": \"3,816\", \"num_start_pos\": 229, \"num_end_pos\": 233, \"measurement\": \"metric tons\", \"meas_start_pos\": 235, \"meas_end_pos\": 245, \"keyword\": \"non-hazardous waste\", \"key_start_pos\": 250, \"key_end_pos\": 268}, \"2\": {\"number\": \"566\", \"num_start_pos\": 38, \"num_end_pos\": 40, \"measurement\": \"metric tons\", \"meas_start_pos\": 42, \"meas_end_pos\": 52, \"keyword\": \"hazardous waste\", \"key_start_pos\": 57, \"key_end_pos\": 71}}}'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = {'success': True,\n",
    "            'output': final_resluts}\n",
    "final_js = json.dumps(response)\n",
    "final_js\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       STATION     STATION_NAME  ELEVATION  LATITUDE  LONGITUDE  \\\n",
      "0  COOP:310301  ASHEVILLE NC US      682.1   35.5954   -82.5568   \n",
      "1  COOP:310301  ASHEVILLE NC US      682.1   35.5954   -82.5568   \n",
      "2  COOP:310301  ASHEVILLE NC US      682.1   35.5954   -82.5568   \n",
      "\n",
      "             DATE   HPCP Measurement Flag Quality Flag  \n",
      "0  20100101 00:00  99999                ]               \n",
      "1  20100101 01:00      0                g               \n",
      "2  20100102 06:00      1                                \n",
      "             a                b      c        d        e               f  \\\n",
      "0  COOP:310301  ASHEVILLE NC US  682.1  35.5954 -82.5568  20100101 00:00   \n",
      "1  COOP:310301  ASHEVILLE NC US  682.1  35.5954 -82.5568  20100101 01:00   \n",
      "2  COOP:310301  ASHEVILLE NC US  682.1  35.5954 -82.5568  20100102 06:00   \n",
      "\n",
      "       g  h  i  \n",
      "0  99999  ]     \n",
      "1      0  g     \n",
      "2      1        \n",
      "[{'a': 'COOP:310301', 'b': 'ASHEVILLE NC US', 'c': 682.1, 'd': 35.5954, 'e': -82.5568, 'f': '20100101 00:00', 'g': 99999, 'h': ']', 'i': ' '}, {'a': 'COOP:310301', 'b': 'ASHEVILLE NC US', 'c': 682.1, 'd': 35.5954, 'e': -82.5568, 'f': '20100101 01:00', 'g': 0, 'h': 'g', 'i': ' '}, {'a': 'COOP:310301', 'b': 'ASHEVILLE NC US', 'c': 682.1, 'd': 35.5954, 'e': -82.5568, 'f': '20100102 06:00', 'g': 1, 'h': ' ', 'i': ' '}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# grabbing a sample file to demonstrate\n",
    "df = pd.read_csv(\"https://www1.ncdc.noaa.gov/pub/data/cdo/samples/PRECIP_HLY_sample_csv.csv\") \n",
    "\n",
    "print(df)  # what it looks on read\n",
    "\n",
    "# if the column names don’t match what’s needed for the api post json,rename them\n",
    "df = df.rename(columns={'STATION':'a','STATION_NAME':'b',\n",
    "                        'ELEVATION':'c','LATITUDE':'d',\n",
    "                        'LONGITUDE':'e','DATE':'f','HPCP':'g',\n",
    "                        'Measurement Flag':'h','Quality Flag':'i'})\n",
    "\n",
    "print(df)  # what it looks like after the rename\n",
    "\n",
    "# render a list of 'json' posts from the dataframe\n",
    "posts = json.loads('{\"items\":' + df.to_json(orient='records', date_format='iso') + '}')\n",
    "\n",
    "print(posts['items']) # see the results\n",
    "\n",
    "# since posts['items'] is a list, the set of posts can be executed using list comprehension like line below\n",
    "# [requests.post('http://someurl.com/path', json=post, headers={'Accept':'application/json'}) for post in posts['items'] ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
